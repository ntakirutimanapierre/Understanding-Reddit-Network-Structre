{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11088442,"sourceType":"datasetVersion","datasetId":6911530}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ntakirutimanapierre/maternal-health?scriptVersionId=228477371\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 0. IMPORT ALL THE PACKAGES","metadata":{}},{"cell_type":"markdown","source":"**DATASET**\nThe dataset can be found here: https://www.kaggle.com/datasets/ntakirutimanapierre/maternal-health-comments-reddit\n\n\nFile size: 14.53GB\n\n\nProcessing in chunks of 1,000,000 rows\n\n9 Column names: Unnamed: 0, author, subreddit, id, created, score, body, parent, link_id\n\n\n                                                            \nFile size: 14536.68 MB\n\n\nTotal rows: 42,238,614\n\n\nColumn count: \n\n\nProcessing time: 418.67 seconds\n\n\nProcessing speed: 34.72 MB/sec","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport numpy as np \nimport pandas as pd\nimport sqlite3\nimport os\nimport re\nimport sys\n\nimport json\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nfrom collections import defaultdict\nimport seaborn as sns\n\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T14:51:23.755376Z","iopub.execute_input":"2025-03-19T14:51:23.755875Z","iopub.status.idle":"2025-03-19T14:51:23.762382Z","shell.execute_reply.started":"2025-03-19T14:51:23.755821Z","shell.execute_reply":"2025-03-19T14:51:23.76056Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **1. READ SAMPLE OF THE DATASET**","metadata":{}},{"cell_type":"code","source":"comments = \"/kaggle/input/maternal-health-comments-reddit/combination/all comments.csv\"\n\n# select the columns you want to read\ncols = ['author', 'subreddit', 'id', 'created', 'score', 'body',\n       'parent', 'link_id']\n# data = pd.read_csv(comments, usecols = cols, nrows = 10000000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:01:28.798445Z","iopub.execute_input":"2025-03-19T16:01:28.79892Z","iopub.status.idle":"2025-03-19T16:01:28.804409Z","shell.execute_reply.started":"2025-03-19T16:01:28.798882Z","shell.execute_reply":"2025-03-19T16:01:28.802979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **2. BUILD INTERACTIVE NETWORK**","metadata":{}},{"cell_type":"code","source":"def build_interaction_network(df, subreddit=None, min_interactions=2):\n    \"\"\"\n    Build a network of user interactions based on replies\n    \n    Parameters:\n    -----------\n    df : pandas DataFrame\n        DataFrame containing Reddit comments\n    subreddit : str, optional\n        If provided, filter to only this subreddit\n    min_interactions : int\n        Minimum number of interactions between users to include in graph\n        \n    Returns:\n    --------\n    G : networkx.Graph\n        Graph representing user interactions\n    \"\"\"\n    # Filter by subreddit if specified\n    if subreddit:\n        df = df[df['subreddit'] == subreddit]\n    \n    \n    \n    # Create a mapping from comment IDs to authors\n    id_to_author = pd.Series(df.author.values, index=df.id).to_dict()\n    \n    # Initialize interaction counter\n    interactions = defaultdict(int)\n    \n    # Count interactions (replies between users)\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building network\"):\n        if pd.isna(row.parent):\n            continue\n            \n        # In the SQLite database, parent_id format may be different than JSON\n        # Common formats: \"t1_...\" for comment replies, \"t3_...\" for post replies\n        parent_id = row.parent.split('_')[1] if '_' in row.parent else row.parent\n        \n        # Get the author of the parent comment\n        parent_author = id_to_author.get(parent_id)\n        \n        # Skip if parent author is not found or if it's the same as the current author\n        # Also skip deleted authors (common in Reddit data)\n        if (not parent_author or \n            parent_author == row.author or \n            parent_author == '[deleted]' or \n            row.author == '[deleted]'):\n            continue\n            \n        # Count the interaction (order alphabetically to avoid duplicates)\n        user_pair = tuple(sorted([row.author, parent_author]))\n        interactions[user_pair] += 1\n    \n    # Create a graph\n    G = nx.Graph()\n    \n    # Add edges for interactions that meet the minimum threshold\n    for (user1, user2), count in interactions.items():\n        if count >= min_interactions:\n            G.add_edge(user1, user2, weight=count)\n    \n    return G","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:01:29.492253Z","iopub.execute_input":"2025-03-19T16:01:29.492681Z","iopub.status.idle":"2025-03-19T16:01:29.504197Z","shell.execute_reply.started":"2025-03-19T16:01:29.492633Z","shell.execute_reply":"2025-03-19T16:01:29.502964Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3. VISUALIZE THE NETWORK**","metadata":{}},{"cell_type":"code","source":"def visualize_network(G, title=\"Reddit User Interaction Network\",\n                      name ='reddit_interaction_network.png', max_nodes=100):\n    \"\"\"\n    Visualize the interaction network\n    \n    Parameters:\n    -----------\n    G : networkx.Graph\n        Graph to visualize\n    title : str\n        Title for the plot\n    max_nodes : int\n        Maximum number of nodes to display\n    \"\"\"\n    if len(G.nodes()) == 0:\n        print(\"No nodes in graph. Try adjusting the filtering parameters.\")\n        return\n        \n    # If graph is too large, take the largest connected component and limit to top nodes by degree\n    if len(G.nodes()) > max_nodes:\n        # Get largest connected component\n        largest_cc = max(nx.connected_components(G), key=len)\n        subG = G.subgraph(largest_cc)\n        \n        # If still too large, filter by degree\n        if len(subG.nodes()) > max_nodes:\n            degrees = dict(nx.degree(subG))\n            top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:max_nodes]\n            subG = G.subgraph(top_nodes)\n        G = subG\n    \n    # Calculate node sizes based on degree centrality\n    degrees = dict(nx.degree(G))\n    node_sizes = [50 + (degrees[node] * 10) for node in G.nodes()]\n    \n    # Calculate edge widths based on weight\n    edge_widths = [G[u][v].get('weight', 1) / 2 for u, v in G.edges()]\n    \n    # Calculate node colors based on betweenness centrality\n    betweenness = nx.betweenness_centrality(G)\n    node_colors = [betweenness[node] for node in G.nodes()]\n    \n    # Set up the plot\n    plt.figure(figsize=(16, 12))\n    pos = nx.spring_layout(G, k=0.3, iterations=50)\n    \n    # Draw the network\n    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, \n                         cmap=plt.cm.viridis, alpha=0.8)\n    nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.3, edge_color='gray')\n    nx.draw_networkx_labels(G, pos, font_size=8, font_family='sans-serif')\n    \n    plt.title(title, fontsize=16)\n    plt.axis('off')\n    plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis), \n               label='Betweenness Centrality')\n    plt.tight_layout()\n    plt.savefig(name, dpi=300)\n    plt.show()\n    \n    return G\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-19T16:10:40.777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <SPAN  STYLE = 'COLOR:RED'> 4. ANALYSE THE NETWORK STRUCTURE AND GENERATE NETWORK STATISTICS </SPAN>","metadata":{}},{"cell_type":"code","source":"def analyze_community_structure(G):\n    \"\"\"\n    Analyze the community structure of the network\n    \n    Parameters:\n    -----------\n    G : networkx.Graph\n        Graph to analyze\n        \n    Returns:\n    --------\n    communities : list\n        List of communities (each is a set of nodes)\n    modularity : float\n        Modularity of the community structure\n    \"\"\"\n    # Detect communities using the Louvain method\n    try:\n        import community as community_louvain\n        partition = community_louvain.best_partition(G)\n        \n        # Group nodes by community\n        communities = defaultdict(set)\n        for node, community_id in partition.items():\n            communities[community_id].add(node)\n        \n        # Calculate modularity\n        modularity = community_louvain.modularity(partition, G)\n        \n        return list(communities.values()), modularity\n    except ImportError:\n        print(\"To analyze community structure, install python-louvain: pip install python-louvain\")\n        return [], 0\n\ndef generate_network_stats(G):\n    \"\"\"\n    Generate statistics about the network\n    \n    Parameters:\n    -----------\n    G : networkx.Graph\n        Graph to analyze\n        \n    Returns:\n    --------\n    stats : dict\n        Dictionary of network statistics\n    \"\"\"\n    try:\n        \n        stats = {\n            'num_nodes': len(G.nodes()),\n            'num_edges': len(G.edges()),\n            'density': nx.density(G),\n            'avg_clustering': nx.average_clustering(G),\n            'diameter': nx.diameter(G) if nx.is_connected(G) else \"Graph is not connected\",\n            'avg_shortest_path': nx.average_shortest_path_length(G) if nx.is_connected(G) else \"Graph is not connected\",\n            'num_connected_components': nx.number_connected_components(G)\n        }\n        \n        # Calculate degree distribution\n        degrees = [d for _, d in G.degree()]\n    \n        stats['max_degree'] = max(degrees) if degrees else 0\n        stats['avg_degree'] = sum(degrees) / len(degrees) if degrees else 0\n        \n    except Exception as e:\n        print(f\"Error:{e}\")\n        return f\"Can't generate network statistics because: {e}\"\n        \n    \n    return stats\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-19T16:10:40.778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. IDENTIFY KEY USERS","metadata":{}},{"cell_type":"code","source":"def identify_key_users(G, top_n=10):\n    \"\"\"\n    Identify key users in the network based on different centrality measures\n    \n    Parameters:\n    -----------\n    G : networkx.Graph\n        Graph to analyze\n    top_n : int\n        Number of top users to return for each measure\n        \n    Returns:\n    --------\n    key_users : dict\n        Dictionary with lists of top users by different measures\n    \"\"\"\n    # Calculate various centrality measures\n    degree_cent = nx.degree_centrality(G)\n    between_cent = nx.betweenness_centrality(G)\n    close_cent = nx.closeness_centrality(G)\n    eigen_cent = nx.eigenvector_centrality_numpy(G)\n    \n    # Get top users for each measure\n    key_users = {\n        'degree': sorted(degree_cent.items(), key=lambda x: x[1], reverse=True)[:top_n],\n        'betweenness': sorted(between_cent.items(), key=lambda x: x[1], reverse=True)[:top_n],\n        'closeness': sorted(close_cent.items(), key=lambda x: x[1], reverse=True)[:top_n],\n        'eigenvector': sorted(eigen_cent.items(), key=lambda x: x[1], reverse=True)[:top_n]\n    }\n    \n    return key_users\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:01:30.39937Z","iopub.execute_input":"2025-03-19T16:01:30.399801Z","iopub.status.idle":"2025-03-19T16:01:30.40756Z","shell.execute_reply.started":"2025-03-19T16:01:30.399764Z","shell.execute_reply":"2025-03-19T16:01:30.406108Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. EXPORT THE NETWORK FOR VISUALIZATION","metadata":{}},{"cell_type":"code","source":"def export_network_for_visualization(G, filename=\"network_data.json\"):\n    \"\"\"\n    Export network data in a format suitable for visualization with D3.js\n    \n    Parameters:\n    -----------\n    G : networkx.Graph\n        Network graph to export\n    filename : str\n        Path to save the JSON file\n    \"\"\"\n    # Get node attributes\n    node_data = []\n    for node, attrs in G.nodes(data=True):\n        # Count comments by this user\n        comment_count = sum(1 for _, target, data in G.edges(node, data=True))\n        \n        # Determine dominant subreddit (if we had that data, not always available)\n        subreddit = \"unknown\"\n        if 'subreddit' in attrs:\n            subreddit = attrs['subreddit']\n        \n        node_data.append({\n            \"id\": node,\n            \"username\": node,\n            \"subreddit\": subreddit,\n            \"commentCount\": comment_count\n        })\n    \n    # Get edge data\n    link_data = []\n    for source, target, attrs in G.edges(data=True):\n        link_data.append({\n            \"source\": source,\n            \"target\": target,\n            \"weight\": attrs.get('weight', 1)\n        })\n    \n    # Create the data object\n    data = {\n        \"nodes\": node_data,\n        \"links\": link_data\n    }\n    \n    # Write to JSON file\n    with open(filename, 'w') as f:\n        json.dump(data, f)\n    \n    print(f\"Network data exported to {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:01:30.71825Z","iopub.execute_input":"2025-03-19T16:01:30.718639Z","iopub.status.idle":"2025-03-19T16:01:30.727235Z","shell.execute_reply.started":"2025-03-19T16:01:30.718599Z","shell.execute_reply":"2025-03-19T16:01:30.726045Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. PUT EVERYTHING TOGETHER AND TEST THE CODES","metadata":{}},{"cell_type":"code","source":"    \ndef main():\n    csv_reader = pd.read_csv(comments, chunksize=1000000, low_memory=True)\n    # Process the file in chunks using a while loop\n    i = 1\n    while True:\n        try:\n            # Read the next chunk\n            chunk = next(csv_reader)\n\n            df = chunk.copy()\n            \n            # Show data summary\n            print(f\"Loaded {len(df)} comments\")\n            print(\"\\nTop subreddits by comment count:\")\n            print(df['subreddit'].value_counts().head(10))\n            \n            # Example: Analyze a specific subreddit (e.g., 'AskReddit')\n            subreddit = 'BabyBumps'\n            print(f\"\\nBuilding interaction network for r/{subreddit}...\")\n            G = build_interaction_network(df, subreddit=subreddit, min_interactions=2)\n            \n            # Network visualization\n            print(f\"\\nVisualizing network with {len(G.nodes())} users and {len(G.edges())} interactions...\")\n            visualize_network(G,name =f'{i}.png', title=f\"User Interaction Network{i} - r/{subreddit}\")\n            \n            # Export network data for interactive visualization\n            print(\"\\nExporting network data for interactive visualization...\")\n            export_network_for_visualization(G, filename=f\"reddit_network{i}.json\")\n            i += 1\n    \n            # Analyze network statistics\n            print(\"\\nNetwork statistics:\")\n            stats = generate_network_stats(G)\n            for key, value in stats.items():\n                print(f\"  {key}: {value}\")\n                \n            \n            # # Identify key users\n            # print(\"\\nKey users by centrality measures:\")\n            # key_users = identify_key_users(G, top_n=5)\n            # for measure, users in key_users.items():\n            #     print(f\"  Top by {measure}:\")\n            #     for user, score in users:\n            #         print(f\"    {user}: {score:.4f}\")\n            \n            # Analyze community structure\n            print(\"\\nAnalyzing community structure...\")\n            communities, modularity = analyze_community_structure(G)\n            print(f\"  Found {len(communities)} communities\")\n            print(f\"  Modularity: {modularity:.4f}\")\n            print(f\"  Largest community size: {max(len(c) for c in communities) if communities else 0}\")\n        except StopIteration:\n            # End of file reached\n            break\nif __name__ == \"__main__\":\n            main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T16:01:31.047728Z","iopub.execute_input":"2025-03-19T16:01:31.048177Z","execution_failed":"2025-03-19T16:10:40.777Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}